{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = pd.read_csv('./data/male_female_words.csv', sep = ';')\n",
    "\n",
    "word_list = word_list.male.tolist() + word_list.female.tolist()\n",
    "word_list = [x for x in word_list if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a20ff9",
   "metadata": {},
   "source": [
    "## Tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_speeches(df, decade, sample_size):\n",
    "    \n",
    "    df = df[df['speech'].map(type) == str]\n",
    "    \n",
    "    party = df.party.iloc[0]\n",
    "    \n",
    "    list_samp_year = []\n",
    "\n",
    "    for year in df.year.unique():\n",
    "        if (year == 2021) & (party == 'Labour'):\n",
    "            list_samp_year.append(df[df['year'] == year])\n",
    "        else:\n",
    "            list_samp_year.append(df[df['year'] == year].sample(n = sample_size))\n",
    "\n",
    "    df_samp_year = pd.concat(list_samp_year)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    samp_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df_samp_year)), total = len(df_samp_year)):\n",
    "        if any(word in df_samp_year.speech.iloc[i].lower() for word in word_list):\n",
    "            samp_list.append(df_samp_year.iloc[i])\n",
    "\n",
    "    df_samp_year_filt = pd.DataFrame(samp_list)\n",
    "    \n",
    "    # TOKENIZATION - lowercase the tokens and remove punctuations\n",
    "\n",
    "    import spacy\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    speeches = df_samp_year_filt.speech\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    processed_speeches = [text for text in tqdm(nlp.pipe(speeches, disable = [\"ner\", \"parser\", \"lemmatizer\"]),\n",
    "                                                total = len(speeches))]\n",
    "\n",
    "    tokenized_speeches = [[word.text.lower() for word in text if not word.is_punct] for text in processed_speeches]\n",
    "\n",
    "    # to remove tokens such as '\\n'\n",
    "    tokenized_speeches = [[re.sub(r'\\W+', '', word) for word in text] for text in tokenized_speeches]\n",
    "\n",
    "    # filter tokenized speeches\n",
    "    tokenized_speeches_filt = []\n",
    "\n",
    "    for i in range(0, len(tokenized_speeches)):\n",
    "        if any(word in tokenized_speeches[i] for word in word_list):\n",
    "                tokenized_speeches_filt.append(tokenized_speeches[i])\n",
    "    \n",
    "    # save filtered tokenized speeches    \n",
    "    with open('./data/output/political/tokenized_speeches_filt_' + party + '_' + decade + '.csv', 'wb') as fp:   #Pickling\n",
    "        pickle.dump(tokenized_speeches_filt, fp)\n",
    "    \n",
    "    print(decade + ' + ' + party + ' is done!')\n",
    "                     \n",
    "    return(tokenized_speeches_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b403cb",
   "metadata": {},
   "source": [
    "### Run tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001812b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decades = ['80_90', '90_00', '00_10', '10_21']\n",
    "parties = ['Conservative', 'Labour']\n",
    "\n",
    "n_sample = 10000\n",
    "\n",
    "for decade in decades:\n",
    "    df = pd.read_csv('./data/speeches_' + decade + '.csv', sep = ',')\n",
    "    \n",
    "    for party in parties:\n",
    "        df_party = df[df['party'] == party]\n",
    "    \n",
    "        tokenize_speeches(df_party, decade, n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4629f86",
   "metadata": {},
   "source": [
    "## Model save function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce82848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_model(decade, party):\n",
    "    \n",
    "    filename = './data/output/political/tokenized_speeches_filt_' + party + '_' + decade + '.csv'\n",
    "\n",
    "    with open(filename, \"rb\") as fp:   # Unpickling\n",
    "        tokenized_speeches_filt = pickle.load(fp)\n",
    "    \n",
    "    # MODEL\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    SIZE      = 300 # dimensions of the embeddings\n",
    "    SG        = 1   # skip-gram\n",
    "    WINDOW    = 10  # window size\n",
    "    N_WORKERS = 1   # number of workers\n",
    "    MIN_COUNT = 5\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = Word2Vec(vector_size = SIZE,\n",
    "                     sg = SG,\n",
    "                     window = WINDOW, \n",
    "                     min_count = MIN_COUNT,\n",
    "                     workers = N_WORKERS)\n",
    "\n",
    "    model.build_vocab(tokenized_speeches_filt)\n",
    "    \n",
    "    model.train(tokenized_speeches_filt,\n",
    "                total_examples = model.corpus_count,\n",
    "                epochs = model.epochs)\n",
    "    \n",
    "    # save model\n",
    "    model.save(\"./models/political/word2vec_\" + party + '_' + decade + \".model\")\n",
    "    \n",
    "    print(decade + ' + ' + party + ' model is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d7374",
   "metadata": {},
   "source": [
    "### Run model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE MODEL\n",
    "\n",
    "decades = ['80_90', '90_00', '00_10', '10_21']\n",
    "parties = ['Conservative', 'Labour']\n",
    "\n",
    "for decade in decades:\n",
    "    for party in parties:\n",
    "        \n",
    "        word2vec_model(decade, party)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
